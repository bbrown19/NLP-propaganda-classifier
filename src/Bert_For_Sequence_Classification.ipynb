{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Bert is Used in Text Classification \n",
    "We will use the pre-trained BERT transformer model and then fine-tune it on propoganda detection. The main benefit of using a Transformer model is that the --------- is multi-directional. You need to look back on your lecture notes on Bert\n",
    "\n",
    "#### HuggingFace \n",
    "One of the best libraries for implementing transofrmers in python. You can install the library with \n",
    "<span style='background:gray'> pip install transformers   </span>. You will also need pytorch installed (pip install torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre Processing the Data \n",
    "To use a pre-trained BERT model the input data must be converted into the required input data format. From there the input data is sent to BERT to obtain the embeddings. Bert embeddings are trained with a classification task and a next sentence prediction task (to determine if a sentence naturally follows the previous one). \n",
    "\n",
    "<b>Classification Task</b> - requires single vector input representing the input sentence. Token [CLS] is chosen. This is what we will be using for our propaganda classification. \n",
    "\n",
    "<b>Next Sentence Prediction Task</b> - where we inform the model where the first sentence ends and the next begins. [SEP] is usd. If each input sample contains only one text input (for classifier) we add [SEP] at the end of the input text. \n",
    "\n",
    "<b>Padding Token</b>: \n",
    "BERT requires a fixed length of text per input. When we have sentences that are shorter than the maximum length, we have to add paddings (empty toknes) to the end of the sentence. Represented by [PAD]\n",
    "\n",
    "<b>Converting Tokens to IDs</b>: \n",
    "BERT has given a unique ID to each token. We need to convert each token in the input sentence into its corresponding unique BERT ID. \n",
    "\n",
    "<b>Note on Vocabulary</b>: Since BERT is a pre-trained model that was trained on a particular corpus, the vocabulary is fixed. Therefore it is possible that some of the words in our data are OOV. We should replace these okens with a special token [UNK] which stansd for the unknown token. BERT also makes use a of WordPiece algoirithm that breaks a word into several subwords. This reduces the number of [UNK] tokens. Without applying the tokenization function of BERT before converting tokens to ids, OOV words will not be split into subwords. \n",
    "\n",
    "<b>Key Steps:</b>\n",
    "1. Applying tokenizer: BertTokenizer.from_pretrained(\"bert-base-cased\").tokenize \n",
    "2. Adding the [CLS] token at the beginning of the sentence \n",
    "3. Adding the [SEP] token at the end of the sentence \n",
    "4. Padding he setence with [PAD] tokens so that the total length equals to the maximum leength \n",
    "4. Converting each token to its corresponding BERT ID using t.convert_tokens_to_ids(tokens)\n",
    "\n",
    "<b>Using Transformers Package:</b>\n",
    "The function encode_plus does all of the above steps at once. Ie. adds the [CLS] & [SEP], sets max length, adds [PADS] and generates an attention mask & IDs in tensor format. The attention mask tells the model which of the tokens are [PAD] tokens and should not be added. Needed for BERT.\n",
    "\n",
    "Q - what is different about tensor format?\n",
    "\n",
    "<b>Distilled BERT:</b>\n",
    "Smaller & Faster version of BERT that retains 95% of BERT performances while only using 60% of the bert-based-uncased parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "import pandas as pd \n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in propoganda data \n",
    "Dataset = pd.read_csv('cleaned_propaganda_data.csv', sep = '|', header = None, names = [\"Text\", \"Label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Split Data into Training, Test & Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the Dataset into train (64%) , validation (16%) and test sets(20%)\n",
    "trainval, test, trainval_labels, test_labels = train_test_split(Dataset['Text'], Dataset['Label'], \n",
    "                                                                    random_state=0, \n",
    "                                                                    test_size=0.2, \n",
    "                                                                    stratify=Dataset['Label'])\n",
    "\n",
    "train, val, train_labels, val_labels = train_test_split(trainval, trainval_labels, \n",
    "                                                                random_state=0, \n",
    "                                                                test_size=0.2, \n",
    "                                                                stratify=trainval_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Tokenize each Dataset to use as Input for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\") #decide if we want to use cased or uncased\n",
    "\n",
    "#Encode each input of text as a unique BERT ID and include CLS, SEP & PAD tokens \n",
    "train_tokens = tokenizer.batch_encode_plus(\n",
    "    train.tolist(),  # the sentence to be encoded\n",
    "    add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "    truncation=True, #truncates sentences longer than max length \n",
    "    max_length = 60,  # maximum length of a sentence -> need to decide what we want this to be \n",
    "    padding=True,  # Add [PAD]s\n",
    "    return_attention_mask = True,  # Generate the attention mask\n",
    "    return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    ")\n",
    "\n",
    "#Validation set encoding \n",
    "val_tokens = tokenizer.batch_encode_plus(\n",
    "    val.tolist(),  # the sentence to be encoded\n",
    "    add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "    truncation=True, #truncates sentences longer than max length \n",
    "    max_length = 60,  # maximum length of a sentence -> need to decide what we want this to be \n",
    "    padding=True,  # Add [PAD]s\n",
    "    return_attention_mask = True,  # Generate the attention mask\n",
    "    return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    ")\n",
    "\n",
    "#Test set encoding\n",
    "test_tokens = tokenizer.batch_encode_plus(\n",
    "    test.tolist(),  # the sentence to be encoded\n",
    "    add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "    truncation=True, #truncates sentences longer than max length \n",
    "    max_length = 60,  # maximum length of a sentence -> need to decide what we want this to be \n",
    "    padding=True,  # Add [PAD]s\n",
    "    return_attention_mask = True,  # Generate the attention mask\n",
    "    return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    ")\n",
    "\n",
    "#Get input_ids, attention_mask, token_type_ids for each dataset & convert labels to tensors\n",
    "with torch.no_grad():\n",
    "    train_ids = train_tokens['input_ids']\n",
    "    train_mask = train_tokens['attention_mask']\n",
    "    train_tt_ids = train_tokens['token_type_ids']\n",
    "    train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "    val_ids = val_tokens['input_ids']\n",
    "    val_mask = val_tokens['attention_mask']\n",
    "    val_tt_ids = val_tokens['token_type_ids']\n",
    "    val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "    test_ids = test_tokens['input_ids']\n",
    "    test_mask = test_tokens['attention_mask']\n",
    "    test_tt_ids = test_tokens['token_type_ids']\n",
    "    test_y = torch.tensor(test_labels.tolist())\n",
    "\n",
    "#Create a tensor dataset for each of the train,test & validation sets \n",
    "train_tensor_data = TensorDataset(train_ids, train_mask, train_tt_ids, train_y) #wrap all tensor data for training\n",
    "val_tensor_data = TensorDataset(val_ids, val_mask, val_tt_ids, val_y)\n",
    "test_tensor_data = TensorDataset(test_ids, test_mask, test_tt_ids, test_y)\n",
    "\n",
    "#Function to create the DataLoaders that load the data in batches \n",
    "def data_loader(train_data, val_data,test_data, batch_size): \n",
    "    \n",
    "    train_sample = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sample, batch_size=batch_size)\n",
    "    \n",
    "    val_sample = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sample, batch_size=batch_size)\n",
    "    \n",
    "    test_sample = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sample, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "#Getting the Dataloaders for each dataset\n",
    "train_dataloader, val_dataloader, test_dataloader = data_loader(train_tensor_data,val_tensor_data, test_tensor_data, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build Bert For Sequence Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Model \n",
    "bertmodel = BertForSequenceClassification.from_pretrained('bert-base-cased')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = bertmodel.to(device) \n",
    "\n",
    "#Define Optimizer - using AdamW \n",
    "optimizer = AdamW(model.parameters(), lr = 5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps for Training: \n",
    "\n",
    "Initially: Define number of epoachs & steps per validation & put model into training mode\n",
    "\n",
    "For each epoch: \n",
    "1. Enumerate through the training dataset dataloader (each batch of data)  \n",
    "2. For each batch: \n",
    "  - push batch to GPU to get the datafields (ids, mask, token_type_ids, labels) \n",
    "  - perform a forward pass of the model to get the output \n",
    "  - get the loss from the ouptut \n",
    "  - remove previous calculated gradients (optimizer.zero_grad())\n",
    "  - perform backwards pass of model to calculate new gradients \n",
    "  - clip gradients to prevent the exploding gradient problem (potential to remove step)\n",
    "  - update parameters in optimizer \n",
    "  - add loss to the running loss \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Bert(model, optimizer, train_loader, val_loader): \n",
    "    \n",
    "    num_epochs = 5\n",
    "    \n",
    "    #initialize values \n",
    "    running_training_loss = 0.0\n",
    "    total_steps = 0\n",
    "    training_loss_list = []\n",
    "    validation_loss_list = []\n",
    "    validation_accuracy_list = []\n",
    "    total_steps_list = []\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs): #for each epoch\n",
    "        model.train() #put model in training mode \n",
    "\n",
    "        for step, batch in enumerate(train_loader): # iterate over batches\n",
    "        \n",
    "            if step % 50 == 0 and not step == 0:# progress update after every 50 batches.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_loader)))\n",
    "\n",
    "            data_ids, mask, tt_ids, labels  = [r.to(device) for r in batch] # push the batch to gpu      \n",
    "\n",
    "            #perform foward pass \n",
    "            output = model(input_ids= data_ids, attention_mask= mask,token_type_ids = tt_ids, labels = labels) # get model predictions for the current batch\n",
    "            loss = output.loss\n",
    "            \n",
    "            optimizer.zero_grad() #remove any previously calculated gradients \n",
    "            loss.backward() # perform backward pass to calculate the gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "\n",
    "            optimizer.step() # update parameters\n",
    "            \n",
    "            # update running values\n",
    "            running_training_loss += loss.item()\n",
    "            total_steps += 1\n",
    "\n",
    "        #Calculate average training loss over the entire dataset for this epoach\n",
    "        average_training_loss = running_training_loss /len(train_loader)\n",
    "        running_training_loss = 0.0\n",
    "\n",
    "        #Measure the performance of the model on the validation dataset after each training epoch \n",
    "        validation_loss, validation_accuracy = evaluate_val(model, val_loader)\n",
    "\n",
    "        training_loss_list.append(average_training_loss)\n",
    "        validation_loss_list.append(validation_loss)\n",
    "        validation_accuracy_list.append(validation_accuracy)\n",
    "        total_steps_list.append(total_steps)\n",
    "\n",
    "        print('Epoch [{}/{}], Training Loss: {:.4f}, Valid Loss: {:.4f}, Validation Accuracy: {:.4f}'\n",
    "                      .format(epoch+1,num_epochs, average_training_loss, validation_loss, validation_accuracy))\n",
    "\n",
    "    print('Finished Training!')\n",
    "    model_metrics = {'training_loss_values': training_loss_list, 'validation_loss_values': validation_loss_list,\n",
    "                     'validation_accuracy_values': validation_accuracy_list, 'total_steps_values': total_steps_list}\n",
    "    return model, model_metrics\n",
    "\n",
    "def evaluate_val(model, val_loader): \n",
    "    model.eval() #put the model into evaluation mode (disables dropout layers)\n",
    "    print(\"evaluation\")\n",
    "\n",
    "    #Accuracy & Loss Variables (will be updated in batches)\n",
    "    validation_running_loss = 0.0\n",
    "    predicted_values = []\n",
    "    true_values = []\n",
    "\n",
    "    with torch.no_grad():                   \n",
    "        for step, batch in enumerate(val_loader):# validation loop\n",
    "            data_ids, mask, tt_ids, labels  = [r.to(device) for r in batch]\n",
    "            output = model(input_ids= data_ids, attention_mask= mask,token_type_ids = tt_ids, labels = labels) # get model predictions for the current batch\n",
    "            loss = output.loss\n",
    "            logits = output.logits\n",
    "\n",
    "            predicted_values.extend(torch.argmax(logits ,dim=1).tolist())\n",
    "            true_values.extend(labels.tolist())\n",
    "            \n",
    "            #Update running loss\n",
    "            validation_running_loss += loss.item()\n",
    "    \n",
    "    #Calculate total loss & accuracy \n",
    "    validation_accuracy = accuracy_score(true_values, predicted_values)\n",
    "    validation_loss = validation_running_loss/len(val_loader)\n",
    "\n",
    "    return validation_loss, validation_accuracy\n",
    "\n",
    "model, model_metrics = train_Bert(model, optimizer, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph of validation loss vs. train loss \n",
    "def get_loss_graph(model_metrics):\n",
    "    training_loss = model_metrics.get(training_loss_values)\n",
    "    validation_loss = model_metrics.get(validation_loss_values)\n",
    "    training_loss = model_metrics.get(training_loss_values)\n",
    "    validation_accuracy = model_metrics.get(validation_accuracy_values)\n",
    "    total_training_steps = model_metrics.get(total_steps_values)\n",
    "\n",
    "    plt.plot(total_training_steps, training_loss, label='Train')\n",
    "    plt.plot(total_training_steps, validation_loss, label='Validation')\n",
    "    plt.xlabel('Training Steps Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show() \n",
    "\n",
    "get_loss_graph(model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of Model \n",
    "def evaluate_model(model, test_loader):\n",
    "    predicted_values = []\n",
    "    true_values = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(test_loader): # iterate over batches\n",
    "        \n",
    "            if step % 50 == 0 and not step == 0:# progress update after every 50 batches.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(test_loader)))\n",
    "\n",
    "            data_ids, mask, tt_ids, labels  = [r.to(device) for r in batch] # push the batch to gpu      \n",
    "\n",
    "            output = model(input_ids= data_ids, attention_mask= mask,token_type_ids = tt_ids, labels = labels) # get model predictions for the current batch\n",
    "            logits = output.logits\n",
    "            predicted_values.extend(torch.argmax(logits,dim=1).tolist())\n",
    "            true_values.extend(labels.tolist())\n",
    "\n",
    "            \n",
    "    \n",
    "    print('Classification Report:')\n",
    "    print(classification_report(true_values, predicted_values, labels=[1,0], digits=4))\n",
    "    \n",
    "    cm = confusion_matrix(true_values, predicted_values, labels=[1,0])\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "\n",
    "    ax.set_title('Confusion Matrix')\n",
    "\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('True Labels')\n",
    "\n",
    "    ax.xaxis.set_ticklabels(['PROPAGANDA', 'REAL NEWS'])\n",
    "    ax.yaxis.set_ticklabels(['PROPAGANDA', 'REAL NEWS'])\n",
    "    \n",
    "\n",
    "evaluate_model(model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
